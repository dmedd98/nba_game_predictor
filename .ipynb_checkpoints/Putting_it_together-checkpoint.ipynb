{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 30)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Function to combine games into one row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_team_games(df, keep_method='home'):\n",
    "    '''Combine a TEAM_ID-GAME_ID unique table into rows by game. Slow.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : Input DataFrame.\n",
    "        keep_method : {'home', 'away', 'winner', 'loser', ``None``}, default 'home'\n",
    "            - 'home' : Keep rows where TEAM_A is the home team.\n",
    "            - 'away' : Keep rows where TEAM_A is the away team.\n",
    "            - 'winner' : Keep rows where TEAM_A is the losing team.\n",
    "            - 'loser' : Keep rows where TEAM_A is the winning team.\n",
    "            - ``None`` : Keep all rows. Will result in an output DataFrame the same\n",
    "                length as the input DataFrame.\n",
    "                \n",
    "        Returns\n",
    "        -------\n",
    "        result : DataFrame\n",
    "    '''\n",
    "    # Join every row to all others with the same game ID.\n",
    "    joined = pd.merge(df, df, suffixes=['_Home', '_Away'],\n",
    "                      on=['SEASON_ID', 'GAME_ID', 'GAME_DATE'])\n",
    "    # Filter out any row that is joined to itself.\n",
    "    result = joined[joined.TEAM_ID_Home != joined.TEAM_ID_Away]\n",
    "    # Take action based on the keep_method flag.\n",
    "    if keep_method is None:\n",
    "        # Return all the rows.\n",
    "        pass\n",
    "    elif keep_method.lower() == 'home':\n",
    "        # Keep rows where TEAM_A is the home team.\n",
    "        result = result[result.MATCHUP_Home.str.contains(' vs. ')]\n",
    "    elif keep_method.lower() == 'away':\n",
    "        # Keep rows where TEAM_A is the away team.\n",
    "        result = result[result.MATCHUP_A.str.contains(' @ ')]\n",
    "    elif keep_method.lower() == 'winner':\n",
    "        result = result[result.WL_A == 'W']\n",
    "    elif keep_method.lower() == 'loser':\n",
    "        result = result[result.WL_A == 'L']\n",
    "    else:\n",
    "        raise ValueError(f'Invalid keep_method: {keep_method}')\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Opening dataframes through pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/pickles/boxscoreadv21.p', 'rb') as readfile: \n",
    "    bs21 = pickle.load(readfile)\n",
    "with open('data/pickles/fourfactors21.p', 'rb') as readfile: \n",
    "    ff21 = pickle.load(readfile)\n",
    "with open('data/pickles/season21.p', 'rb') as readfile: \n",
    "    season21 = pickle.load(readfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/pickles/boxscoreadv20.p', 'rb') as readfile: \n",
    "    bs20 = pickle.load(readfile)\n",
    "with open('data/pickles/fourfactors20.p', 'rb') as readfile: \n",
    "    ff20 = pickle.load(readfile)\n",
    "with open('data/pickles/season20.p', 'rb') as readfile: \n",
    "    season20 = pickle.load(readfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/pickles/boxscoreadv19.p', 'rb') as readfile: \n",
    "    bs19 = pickle.load(readfile)\n",
    "with open('data/pickles/fourfactors19.p', 'rb') as readfile: \n",
    "    ff19 = pickle.load(readfile)\n",
    "with open('data/pickles/season19.p', 'rb') as readfile: \n",
    "    season19 = pickle.load(readfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/pickles/boxscoreadv18.p', 'rb') as readfile: \n",
    "    bs18 = pickle.load(readfile)\n",
    "with open('data/pickles/fourfactors18.p', 'rb') as readfile: \n",
    "    ff18 = pickle.load(readfile)\n",
    "with open('data/pickles/season18.p', 'rb') as readfile: \n",
    "    season18 = pickle.load(readfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/pickles/boxscoreadv17.p', 'rb') as readfile: \n",
    "    bs17 = pickle.load(readfile)\n",
    "with open('data/pickles/fourfactors17.p', 'rb') as readfile: \n",
    "    ff17 = pickle.load(readfile)\n",
    "with open('data/pickles/season17.p', 'rb') as readfile: \n",
    "    season17 = pickle.load(readfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nba_api.stats.endpoints import boxscorefourfactorsv2\n",
    "fix = boxscorefourfactorsv2.BoxScoreFourFactorsV2(game_id = '0022001069')\n",
    "fixdf = fix.get_data_frames()[1]\n",
    "\n",
    "\n",
    "ff21.reset_index(drop=True, inplace=True)\n",
    "\n",
    "ff21.loc[0:1]=fixdf"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Function to combine all three dataframe for each year into one large df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dfs(season_df, bs_df, ff_df):\n",
    "    merge1 = pd.merge(season_df, bs_df, on=['GAME_ID', 'TEAM_ID']).drop(labels=['TEAM_NAME_y', 'TEAM_ABBREVIATION_y', 'MIN_y', 'TEAM_CITY'], axis=1)\n",
    "    merge2 = pd.merge(merge1, ff_df, on = ['GAME_ID', 'TEAM_ID']).drop(labels=['TEAM_NAME', 'TEAM_ABBREVIATION', 'TEAM_CITY', 'MIN','EFG_PCT_y', 'TM_TOV_PCT_y'], axis=1)\n",
    "    df = combine_team_games(merge2, keep_method='home')\n",
    "    df.set_index(pd.to_datetime(df['GAME_DATE']), drop=True, inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This function will do the same but not combine games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dfs_seperate(season_df, bs_df, ff_df):\n",
    "    merge1 = pd.merge(season_df, bs_df, on=['GAME_ID', 'TEAM_ID']).drop(labels=['TEAM_NAME_y', 'TEAM_ABBREVIATION_y', 'MIN_y', 'TEAM_CITY'], axis=1)\n",
    "    merge2 = pd.merge(merge1, ff_df, on = ['GAME_ID', 'TEAM_ID']).drop(labels=['TEAM_NAME', 'TEAM_ABBREVIATION', 'TEAM_CITY', 'MIN','EFG_PCT_y', 'TM_TOV_PCT_y'], axis=1)\n",
    "    merge2.set_index(pd.to_datetime(merge2['GAME_DATE']), drop=True, inplace=True)\n",
    "    merge2.sort_index(inplace=True)\n",
    "    return merge2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Save and pickle combined dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitdf21 = combine_dfs_seperate(season21, bs21, ff21)\n",
    "splitdf20 = combine_dfs_seperate(season20, bs20, ff20)\n",
    "splitdf19 = combine_dfs_seperate(season19, bs19, ff19)\n",
    "splitdf18 = combine_dfs_seperate(season18, bs18, ff18)\n",
    "splitdf17 = combine_dfs_seperate(season17, bs17, ff17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/pickles/splitdf21.p', 'wb') as writefile: \n",
    "    pickle.dump(splitdf21, writefile)\n",
    "    \n",
    "with open('data/pickles/splitdf20.p', 'wb') as writefile: \n",
    "    pickle.dump(splitdf20, writefile)\n",
    "    \n",
    "with open('data/pickles/splitdf19.p', 'wb') as writefile: \n",
    "    pickle.dump(splitdf19, writefile)\n",
    "    \n",
    "with open('data/pickles/splitdf18.p', 'wb') as writefile: \n",
    "    pickle.dump(splitdf18, writefile)\n",
    "    \n",
    "with open('data/pickles/splitdf17.p', 'wb') as writefile: \n",
    "    pickle.dump(splitdf17, writefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df21 = combine_dfs(season21, bs21, ff21)\n",
    "df20 = combine_dfs(season20, bs20, ff20)\n",
    "df19 = combine_dfs(season19, bs19, ff19)\n",
    "df18 = combine_dfs(season18, bs18, ff18)\n",
    "df17 = combine_dfs(season17, bs17, ff17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/pickles/df21.p', 'wb') as writefile: \n",
    "    pickle.dump(df21, writefile)\n",
    "    \n",
    "with open('data/pickles/df20.p', 'wb') as writefile: \n",
    "    pickle.dump(df20, writefile)\n",
    "    \n",
    "with open('data/pickles/df19.p', 'wb') as writefile: \n",
    "    pickle.dump(df19, writefile)\n",
    "    \n",
    "with open('data/pickles/df18.p', 'wb') as writefile: \n",
    "    pickle.dump(df18, writefile)\n",
    "    \n",
    "with open('data/pickles/df17.p', 'wb') as writefile: \n",
    "    pickle.dump(df17, writefile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PLUS_MINUS_Home      1.000000\n",
       "NET_RATING_Home      0.993704\n",
       "E_NET_RATING_Home    0.976128\n",
       "PIE_Home             0.938534\n",
       "E_OFF_RATING_Home    0.614020\n",
       "E_DEF_RATING_Away    0.614020\n",
       "DEF_RATING_Away      0.606194\n",
       "OFF_RATING_Home      0.606194\n",
       "TS_PCT_Home          0.561642\n",
       "EFG_PCT_x_Home       0.546302\n",
       "OPP_EFG_PCT_Away     0.546302\n",
       "PTS_Home             0.543001\n",
       "FG_PCT_Home          0.512827\n",
       "FGM_Home             0.460586\n",
       "REB_PCT_Home         0.442423\n",
       "DREB_Home            0.417182\n",
       "AST_RATIO_Home       0.406193\n",
       "AST_Home             0.402756\n",
       "FG3_PCT_Home         0.385150\n",
       "FG3M_Home            0.348831\n",
       "Name: PLUS_MINUS_Home, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
